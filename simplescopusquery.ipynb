{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file runs a simple Scopus query and outputs scopusIDs to a CSV file.\n",
    "\n",
    "Getting information at an article level is a two stage process.  The first is to run a search, and get a list of article IDs that match it.  The second is to run each article ID as a query agains Scopus (or another index if there are DOIs) to get all the metadata.\n",
    "\n",
    "The file [scopussearch.html](scopussearch.html) has documentation for all the ways to search scopus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Libraries** Import all the libraries we are going to need.\n",
    "* **apikey** is a esperate file that has a variable apikey = _your api key_\n",
    "* **requests** makes submitting a query over HTTP straightforward\n",
    " * [requests documentation](https://requests.readthedocs.io/en/latest/user/quickstart/)\n",
    "* **pprint** formats output nicely while debugging\n",
    "* **json** helps to convert json data from scopus into formats we can output and put into other programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apikey import apikey # this imports your elsevier api key\n",
    "import requests\n",
    "import pprint\n",
    "import json\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constants** here are some things that will remain constant throughout the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemurl =\"http://api.elsevier.com/content/search/scopus\" # the standard entrypoint for the Scopus API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Query** This is the query we are going to run against Scopus.  See [scopus search fields documentation](scopussearchfields.html) for more things you can search for.  For DOIs, `doi('10.26021/10274')` would do the trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"all('Oamaru')\" #searching for the phrase 'Oamaru'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query scopus using the requests library.  This will get the first result as a test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://api.elsevier.com/content/search/scopusall('Oamaru')\n"
     ]
    }
   ],
   "source": [
    "fullquery = query #add anything to the query at the last minute\n",
    "print(stemurl+fullquery) #this is all the information being sent to Scopus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block uses the **Requests** library to make the call to the scopus api.  Firstly we create a dictionary 'payload' of all our options, and send that to the main URL.  It handles all the URL serialisation and escaping.\n",
    "\n",
    "At this point we are only going to get _one_ result. (see `count: 1` in the payload)  This lets us run and test our query before letting it loose on Scopus, and using up all our query quota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'query': fullquery, 'apiKey':apikey, 'httpAccept':'application/json', 'cursor':'*', 'count': 1} #create a python dictionary that holds all the parameters that will go into the query\n",
    "r = requests.get(stemurl, params=payload) # this uses requests to get data\n",
    "text = json.loads(r.text) # get the text of the response in JSON format, and put it in a python dictponary object so we can have a squizz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Debug** Lets have a look at what we have got from the API to see if it looks like what we want.  We are putting the Python JSON object through prettyprint.  Uncomment this (remove the `#` hash marks from the pp.pprint line and one of the lines that begins ```results```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'10.1007/s10347-020-00619-4'\n"
     ]
    }
   ],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4) # create a prettyprint method\n",
    "#results = text #everything including headers\n",
    "#results = text['search-results']['entry']# just the entries\n",
    "results = text['search-results']['entry'][0]['prism:doi']#just the keys\n",
    "pp.pprint(results) #lets see the result.\n",
    "\n",
    "# print(r.headers['X-RateLimit-Remaining']) # If you're worried about running out of queries with Scopus (2000 a day?) uncomment this and it will tell you how many queries you can make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many total results there were from our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "679\n"
     ]
    }
   ],
   "source": [
    "resultsTotal = int(text['search-results']['opensearch:totalResults'])\n",
    "print(resultsTotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets grab the ScopusIDs and DOIs of each entry.  They are in ```text['search-results']['entry'][?]['dc:identifier']:``` (check the debug output above for thefull results to have a look at the data structure returned) and put them in a result file, named with the date and time, in a directory called 'results'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resultsfilename = \"results/queryresults-\" + time.strftime(\"%Y%m%d-%H%M%S\") +'.tsv'# a string made of the date and time.\n",
    "resultsfile = open(resultsfilename, \"a\")\n",
    "\n",
    "headers = 'scopusID \\t DOI \\n'\n",
    "resultsfile.write(headers)\n",
    "firstentryid = text['search-results']['entry'][0]['dc:identifier'] \n",
    "firstentryDOI = text['search-results']['entry'][0]['prism:doi'] \n",
    "resultsfile.write(firstentryid + '\\t'+ firstentryDOI + '\\n')\n",
    "resultsfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have our first result, so the remaining results are one less than that, and we can start our main query (if everything is OK).  We'll get 200 results at a time (that as many as scopus let you get).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(payload['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478 19601\n",
      "278 19600\n",
      "78 19599\n",
      "-122 19598\n"
     ]
    }
   ],
   "source": [
    "resultsRemaining = resultsTotal -1 # we already have the first one\n",
    "while resultsRemaining > 0 :\n",
    "\n",
    "    payload['cursor'] = text['search-results']['cursor']['@next'] #scopus embeds the next url in the results, we can dig it out and use it here.\n",
    "    payload['count'] = 200\n",
    "    \n",
    "    r = requests.get(stemurl, params=payload) # this uses requests to get data\n",
    "    remainingrequests = r.headers['X-RateLimit-Remaining'] #how many more requests can we make today?\n",
    "    \n",
    "    text = json.loads(r.text) # get the text of the response in JSON format, and put it in a python dictionary object so we can have a squizz\n",
    "    \n",
    "    for entry in text['search-results']['entry']:\n",
    "        # print(entry['dc:identifier']) #print the scopus identifier, a tab, then the doi.\n",
    "        resultsfile = open(resultsfilename, \"a\")\n",
    "        entryid = entry['dc:identifier']\n",
    "        if 'prism:doi' in entry: \n",
    "            entrydoi = entry['prism:doi']\n",
    "        else: \n",
    "            entrydoi = 'doi error'\n",
    "        resultsfile.write(entryid+'\\t'+entrydoi+'\\n') #throw everything in the results file\n",
    "        resultsfile.close()  \n",
    "    resultsRemaining = resultsRemaining - 200\n",
    "    print(str(resultsRemaining) , str(remainingrequests)) # how many resukts left to do, how many queries you have left for this key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
